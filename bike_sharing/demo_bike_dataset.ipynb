{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859dd9d6",
   "metadata": {},
   "source": [
    "# Bike Sharing Dataset Exogenous Variables Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b95f0",
   "metadata": {},
   "source": [
    "In this notebook, you will be guided on how to leverage the exogenous variables we provide in order to improve the performance of the bike-sharing dataset. The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e25bc8",
   "metadata": {},
   "source": [
    "## Standard imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de180471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:45:32.009161Z",
     "start_time": "2023-05-18T07:45:31.588833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings, requests\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc54ff",
   "metadata": {},
   "source": [
    "## Download the bulk dataset from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6c4a7",
   "metadata": {},
   "source": [
    "Enter your API key to get access to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78eba28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:32:00.582083Z",
     "start_time": "2023-05-18T07:31:49.036078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Notional API key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "api_key = getpass.getpass('Enter your Notional API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daefd4f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:39:59.638915Z",
     "start_time": "2023-05-18T07:39:59.057279Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://api.notional.ai/v1/series/bulk\"\n",
    "headers = {\n",
    "  \"x-notionalai-api-key\": api_key,\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d86ab00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:44:51.844416Z",
     "start_time": "2023-05-18T07:42:35.062297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 99%"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./data/all_features.parquet', <http.client.HTTPMessage at 0x7fc122b6e040>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "def download_progress(count, block_size, total_size):\n",
    "    if np.random.rand() > 0.8: # Limit the number of printing in notebook\n",
    "        percent = int(count * block_size * 100 / total_size)\n",
    "        sys.stdout.write('\\rDownloading: %d%%' % percent)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "urllib.request.install_opener(opener)\n",
    "urllib.request.urlretrieve(response.json()['result_url'], './data/all_features.parquet', reporthook=download_progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda1757",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc6ac1",
   "metadata": {},
   "source": [
    "Define the loss function that you want. Here we will use root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447a76d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:45:32.977208Z",
     "start_time": "2023-05-18T07:45:32.574088Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e55c0",
   "metadata": {},
   "source": [
    "Your dataset should be in tabular format with a `date/timestamp column` and a `target column`. All other columns will be considered as exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9351e202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:45:33.657287Z",
     "start_time": "2023-05-18T07:45:33.648421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose your lost function here\n",
    "scoring = rmse\n",
    "\n",
    "# Date/timestamp column\n",
    "timestamp_col = 'date'\n",
    "\n",
    "# Target column, i.e label\n",
    "target_col = 'count'\n",
    "\n",
    "# Path to the bulk dataset parquet file\n",
    "features_parquet_path = 'data/all_features.parquet'\n",
    "\n",
    "# The directory to store the feature evaluation results\n",
    "output_dir = 'fs_results'\n",
    "\n",
    "# Number of trials for optuna hyperparameter tuning\n",
    "optuna_n_trials = 10\n",
    "\n",
    "# Forecast length\n",
    "prediction_length = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbad686",
   "metadata": {},
   "source": [
    "We'll read and split the dataset into a train set and a test set. In addition, we will also get the `cvs` variable, which contain the validation split for our cross validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92695271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T07:45:36.761344Z",
     "start_time": "2023-05-18T07:45:35.375735Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import prepare_train_val_test_data\n",
    "\n",
    "# Read the bike sharing dataset\n",
    "data = pd.read_csv(\"data/bike_sharing_day.csv\")\n",
    "\n",
    "# Currenly we support the timestamp column as string type\n",
    "data[timestamp_col] = data[timestamp_col].astype(str)\n",
    "data = data.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "# The length of the test dataset. Set it to None for it to equals to the prediction_length\n",
    "test_size = None\n",
    "\n",
    "# The ratio of the validation dataset used for cross validation\n",
    "val_ratio = 0.25\n",
    "\n",
    "# Number of cross validation folds\n",
    "cv_fold = 5\n",
    "\n",
    "# Should we add a lag_<prediction_length> column to the dataset? Should be yes in most of the cases.\n",
    "add_lag_col = True\n",
    "\n",
    "train_data, test_data, cvs = prepare_train_val_test_data(\n",
    "    data=data, \n",
    "    target_col=target_col, \n",
    "    timestamp_col=timestamp_col, \n",
    "    test_size=test_size, \n",
    "    val_ratio=val_ratio, \n",
    "    cv_fold=cv_fold, \n",
    "    prediction_length=prediction_length, \n",
    "    add_lag_col=add_lag_col\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3e66e",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf9580",
   "metadata": {},
   "source": [
    "Our feature selection method consists of multiple steps to ensure significant improvement and the applicability of selected features to a wide range of time series forecasting models, even though the method is built solely on the XGBoost model. To utilize our feature selection method, follow these steps:\n",
    "\n",
    "1. Create an instance of the FeatureSelector class.\n",
    "2. Call the `fit` method on the created instance, providing the necessary parameters. Note that this process requires a machine with a GPU.\n",
    "3. Once the feature selection process is completed, you can use the get_best_features() method to obtain a list of features with strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de89864d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T05:21:35.289471Z",
     "start_time": "2023-05-18T05:21:35.279937Z"
    }
   },
   "outputs": [],
   "source": [
    "from feature_selection import FeatureSelector\n",
    "feature_selector = FeatureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23091653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T05:24:22.947887Z",
     "start_time": "2023-05-18T05:21:35.996825Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f145d23373477dace3672d9aab4724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      " 11%|█         | 330/2987 [02:32<20:25,  2.17it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"calculate_feature_score.py\", line 43, in <module>\n",
      "    output = calculate_feature_score(**input_dict)\n",
      "  File \"calculate_feature_score.py\", line 22, in calculate_feature_score\n",
      "    losses = run_cv(model, train_data_exo_small, target_col,\n",
      "  File \"/notional_data/phuc_workspace/notional-ts-examples/utils.py\", line 197, in run_cv\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/sklearn.py\", line 789, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/core.py\", line 1677, in update\n",
      "    self._validate_features(dtrain)\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/core.py\", line 2465, in _validate_features\n",
      "    if data.feature_names is None and self.feature_names is not None:\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/core.py\", line 967, in feature_names\n",
      "    _check_call(\n",
      "  File \"/opt/conda/envs/notional-ts/lib/python3.8/site-packages/xgboost/core.py\", line 206, in _check_call\n",
      "    def _check_call(ret):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptuna_n_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptuna_n_trials\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notional_data/phuc_workspace/notional-ts-examples/feature_selection.py:176\u001b[0m, in \u001b[0;36mFeatureSelector.fit\u001b[0;34m(self, train_data, cvs, timestamp_col, target_col, prediction_length, parquet_file_path, output_dir, scoring, optuna_n_trials)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptuna_n_trials \u001b[38;5;241m=\u001b[39m optuna_n_trials\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__finetune_xgb()\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__run_feature_selection_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m top_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_top_10_features()\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(top_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/notional_data/phuc_workspace/notional-ts-examples/feature_selection.py:94\u001b[0m, in \u001b[0;36mFeatureSelector.__run_feature_selection_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m     88\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalculate_feature_score.py\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     89\u001b[0m     stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     90\u001b[0m     stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     91\u001b[0m     bufsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m cloudpickle\u001b[38;5;241m.\u001b[39mdump(input_dict, process\u001b[38;5;241m.\u001b[39mstdin)\n\u001b[0;32m---> 94\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_selector.fit(\n",
    "    train_data=train_data,\n",
    "    cvs=cvs,\n",
    "    timestamp_col=timestamp_col,\n",
    "    target_col=target_col,\n",
    "    prediction_length=prediction_length,\n",
    "    features_parquet_path=features_parquet_path,\n",
    "    output_dir=output_dir,\n",
    "    scoring=scoring,\n",
    "    optuna_n_trials=optuna_n_trials,\n",
    "    gpu_id=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18441e2e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T04:51:22.708Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_features = feature_selector.get_best_features()\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b275080",
   "metadata": {},
   "source": [
    "# Evaluate performance of selected features on different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb6ce8",
   "metadata": {},
   "source": [
    "Import necessary modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740d5ac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T04:51:22.709Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from utils import ARIMAModel\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils import fine_tune_model, evaluate_models, add_exo_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b52fae",
   "metadata": {},
   "source": [
    "We will assess the performance of our selected features using four different time series forecasting models: SARIMAX, Lasso, XGBoost, and RandomForest. This evaluation aims to determine the robustness of the selected features.\n",
    "\n",
    "We will evaluate the model performance with and without the selected features and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10707b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T04:51:22.711Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_tune_model_args = {\n",
    "    'train_data': train_data, \n",
    "    'target_col': target_col, \n",
    "    'cvs': cvs, \n",
    "    'scoring': scoring, \n",
    "    'timestamp_col': timestamp_col, \n",
    "    'optuna_n_trials': optuna_n_trials\n",
    "}\n",
    "\n",
    "arima_model = ARIMAModel()\n",
    "lasso_model = fine_tune_model('lasso', **fine_tune_model_args)\n",
    "xgb_model = fine_tune_model('xgboost', **fine_tune_model_args)\n",
    "rf_model = fine_tune_model('random_forest', **fine_tune_model_args)\n",
    "\n",
    "models = [arima_model, lasso_model, xgb_model, rf_model]\n",
    "evaluate_models(models, train_data, test_data, target_col, timestamp_col, scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c2c2b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T04:51:22.716Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_final = add_exo_features(\n",
    "    train_data, \n",
    "    timestamp_col, \n",
    "    final_selected_features, \n",
    "    parquet_file_path, \n",
    "    prediction_length\n",
    ")\n",
    "\n",
    "test_data_final = add_exo_features(\n",
    "    test_data, \n",
    "    timestamp_col, \n",
    "    final_selected_features, \n",
    "    parquet_file_path, \n",
    "    prediction_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11361277",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T04:51:22.718Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fine_tune_model_args = {\n",
    "    'train_data': train_data_final, \n",
    "    'target_col': target_col, \n",
    "    'cvs': cvs, \n",
    "    'scoring': scoring, \n",
    "    'timestamp_col': timestamp_col, \n",
    "    'optuna_n_trials': optuna_n_trials\n",
    "}\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "arima_model = ARIMAModel()\n",
    "lasso_model = fine_tune_model('lasso', **fine_tune_model_args)\n",
    "xgb_model = fine_tune_model('xgboost', **fine_tune_model_args)\n",
    "rf_model = fine_tune_model('random_forest', **fine_tune_model_args)\n",
    "\n",
    "models = [lr_model, arima_model, lasso_model, xgb_model, rf_model]\n",
    "evaluate_models(models, train_data_final, test_data_final, target_col, timestamp_col, scoring)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
