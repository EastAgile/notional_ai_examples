{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1277bcbd",
   "metadata": {},
   "source": [
    "# Bike Sharing Dataset Exogenous Variables Selection "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9d76d0",
   "metadata": {},
   "source": [
    "In this notebook, you will be guided on how to leverage the exogenous variables we provide in order to improve the performance of the bike-sharing dataset. The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "892ceed8",
   "metadata": {},
   "source": [
    "Copy the code to the current working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db802749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T03:14:16.508352Z",
     "start_time": "2023-05-25T03:14:14.755182Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cp ../calculate_feature_score.py .\n",
    "!cp ../feature_selection.py .\n",
    "!cp ../utils.py ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a5aeada",
   "metadata": {},
   "source": [
    "## Standard imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de180471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T17:03:30.742597Z",
     "start_time": "2023-05-25T17:03:30.319574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, warnings, requests, sys\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0bd362",
   "metadata": {},
   "source": [
    "## Download the bulk dataset from the API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43230105",
   "metadata": {},
   "source": [
    "Enter your API key to get access to the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97ed4989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T02:21:53.101673Z",
     "start_time": "2023-05-25T02:21:25.781124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Notional API key: ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "api_key = getpass.getpass('Enter your Notional API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea29945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T02:21:55.052254Z",
     "start_time": "2023-05-25T02:21:54.300794Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://api.notional.ai/v1/series/bulk\"\n",
    "headers = {\n",
    "  \"x-notionalai-api-key\": api_key,\n",
    "}\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aec3447f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T02:53:38.266469Z",
     "start_time": "2023-05-25T02:21:55.058564Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "urllib.request.install_opener(opener)\n",
    "urllib.request.urlretrieve(response.json()['result_url'], './data/all_features.parquet');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bda1757",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "598aa76c",
   "metadata": {},
   "source": [
    "Define the loss function that you want. Here we will use root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447a76d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T17:03:35.414693Z",
     "start_time": "2023-05-25T17:03:35.001963Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b57b43c0",
   "metadata": {},
   "source": [
    "Your dataset should be in tabular format with a `date/timestamp column` and a `target column`. All other columns will be considered as exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9351e202",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T17:03:44.020574Z",
     "start_time": "2023-05-25T17:03:44.012961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose your lost function here\n",
    "scoring = rmse\n",
    "\n",
    "# Date/timestamp column\n",
    "timestamp_col = 'date'\n",
    "\n",
    "# Target column, i.e label\n",
    "target_col = 'count'\n",
    "\n",
    "# Path to the bulk dataset parquet file\n",
    "features_parquet_path = 'data/all_features.parquet'\n",
    "\n",
    "# The directory to store the feature evaluation results\n",
    "output_dir = 'fs_results'\n",
    "\n",
    "# Number of trials for optuna hyperparameter tuning\n",
    "optuna_n_trials = 200\n",
    "\n",
    "# Forecast length\n",
    "prediction_length = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b2e1267",
   "metadata": {},
   "source": [
    "We'll read and split the dataset into a train set and a test set. In addition, we will also get the `cvs` variable, which contain the validation split for our cross validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66231757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T17:03:48.860664Z",
     "start_time": "2023-05-25T17:03:47.422411Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import prepare_train_val_test_data\n",
    "\n",
    "# Read the bike sharing dataset\n",
    "data = pd.read_csv(\"data/bike_sharing_day.csv\")\n",
    "\n",
    "# Currenly we support the timestamp column as string type\n",
    "data[timestamp_col] = data[timestamp_col].astype(str)\n",
    "data = data.sort_values(timestamp_col).reset_index(drop=True)\n",
    "\n",
    "# The length of the test dataset. Set it to None for it to equals to the prediction_length\n",
    "test_size = None\n",
    "\n",
    "# The ratio of the validation dataset used for cross validation\n",
    "val_ratio = 0.25\n",
    "\n",
    "# Number of cross validation folds\n",
    "cv_fold = 5\n",
    "\n",
    "# Should we add a lag_<prediction_length> column to the dataset? Should be yes in most of the cases.\n",
    "add_lag_col = True\n",
    "\n",
    "train_data, test_data, cvs = prepare_train_val_test_data(\n",
    "    data=data, \n",
    "    target_col=target_col, \n",
    "    timestamp_col=timestamp_col, \n",
    "    test_size=test_size, \n",
    "    val_ratio=val_ratio, \n",
    "    cv_fold=cv_fold, \n",
    "    prediction_length=prediction_length, \n",
    "    add_lag_col=add_lag_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3701c52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T10:07:25.567898Z",
     "start_time": "2023-05-25T10:07:25.560009Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['count'] = np.log1p(train_data['count'])\n",
    "test_data['count'] = np.log1p(test_data['count'])\n",
    "train_data['count_lag_14'] = np.log1p(train_data['count_lag_14'])\n",
    "test_data['count_lag_14'] = np.log1p(test_data['count_lag_14'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec3e66e",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a4d9f0c",
   "metadata": {},
   "source": [
    "Our feature selection method consists of multiple steps to ensure significant improvement and the applicability of selected features to a wide range of time series forecasting models, even though the method is built solely on the XGBoost model. To utilize our feature selection method, follow these steps:\n",
    "\n",
    "1. Create an instance of the FeatureSelector class.\n",
    "2. Call the `fit` method on the created instance, providing the necessary parameters. Note that this process requires a machine with a GPU.\n",
    "3. Once the feature selection process is completed, you can use the get_best_features() method to obtain a list of features with strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de89864d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-25T17:04:01.311425Z",
     "start_time": "2023-05-25T17:04:01.304029Z"
    }
   },
   "outputs": [],
   "source": [
    "from feature_selection import FeatureSelector\n",
    "feature_selector = FeatureSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23091653",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T04:41:38.067838Z",
     "start_time": "2023-05-26T04:32:17.675849Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_selector.fit(\n",
    "    train_data=train_data,\n",
    "    cvs=cvs,\n",
    "    timestamp_col=timestamp_col,\n",
    "    target_col=target_col,\n",
    "    prediction_length=prediction_length,\n",
    "    features_parquet_path=features_parquet_path,\n",
    "    output_dir=output_dir,\n",
    "    scoring=scoring,\n",
    "    optuna_n_trials=optuna_n_trials,\n",
    "    gpu_id=0,\n",
    "    fitted=False #Refit?\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43575531",
   "metadata": {},
   "source": [
    "Once the `feature_selector.fit()` method is invoked, it generates a directory named according to the `output_dir` variable to hold the evaluation results. In the future, if you wish to utilize the stored results without retraining, you can simply employ `feature_selector.fit()` with the fitted=True parameter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95253d7e",
   "metadata": {},
   "source": [
    "Get top 5 features subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18441e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T02:00:23.887589Z",
     "start_time": "2023-05-26T02:00:23.872262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WCR_PRCP_00000267', 'WCR_PRCP_00000263'],\n",
       " ['WCR_PRCP_00002702', 'WCR_PRCP_00000867', 'WCR_PRCP_00000270'],\n",
       " ['WCR_PRCP_00000267'],\n",
       " ['WSR_PRCP_00000051'],\n",
       " ['WCR_PRCP_00000270']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_features = feature_selector.get_n_best_features(5)\n",
    "selected_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b275080",
   "metadata": {},
   "source": [
    "# Evaluate performance of selected features on different models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6ba550f",
   "metadata": {},
   "source": [
    "Import necessary modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9740d5ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T02:00:38.695555Z",
     "start_time": "2023-05-26T02:00:38.689105Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from utils import ARIMAModel\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils import fine_tune_model, evaluate_models, add_exo_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f8c31f7",
   "metadata": {},
   "source": [
    "We will assess the performance of our selected features using four different time series forecasting models: SARIMAX, Lasso, XGBoost, and RandomForest. This evaluation aims to determine the robustness of the selected features.\n",
    "\n",
    "We will evaluate the model performance with and without the selected features and make comparisons. To ensure the reliability of the evaluation, it will be conducted on unseen data, guaranteeing the significance and generalizability of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed10707b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T02:32:34.454808Z",
     "start_time": "2023-05-26T02:27:40.186335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ARIMAModel\n",
      "Loss: 1485.3088843066928\n",
      "Model name: Lasso\n",
      "Loss: 1809.4064113960876\n",
      "Model name: XGBRegressor\n",
      "Loss: 1517.2778732478303\n",
      "==============================\n",
      "Best model: ARIMAModel\n",
      "Best loss: 1485.3088843066928\n"
     ]
    }
   ],
   "source": [
    "fine_tune_model_args = {\n",
    "    'train_data': train_data, \n",
    "    'target_col': target_col, \n",
    "    'cvs': cvs, \n",
    "    'scoring': scoring, \n",
    "    'timestamp_col': timestamp_col, \n",
    "    'optuna_n_trials': optuna_n_trials\n",
    "}\n",
    "\n",
    "arima_model = ARIMAModel()\n",
    "lasso_model = fine_tune_model('lasso', **fine_tune_model_args)\n",
    "xgb_model = fine_tune_model('xgboost', **fine_tune_model_args)\n",
    "# rf_model = fine_tune_model('random_forest', **fine_tune_model_args)\n",
    "\n",
    "models = [arima_model, lasso_model, xgb_model]\n",
    "evaluate_models(models, train_data, test_data, target_col, timestamp_col, scoring, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "986c2c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T02:32:41.308430Z",
     "start_time": "2023-05-26T02:32:34.457398Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_final = add_exo_features(\n",
    "    train_data, \n",
    "    timestamp_col, \n",
    "    selected_features[0], \n",
    "    features_parquet_path, \n",
    "    prediction_length\n",
    ")\n",
    "\n",
    "test_data_final = add_exo_features(\n",
    "    test_data, \n",
    "    timestamp_col, \n",
    "    selected_features[0], \n",
    "    features_parquet_path, \n",
    "    prediction_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11361277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T02:37:51.099736Z",
     "start_time": "2023-05-26T02:32:41.310371Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ARIMAModel\n",
      "Loss: 1638.281730928382\n",
      "Model name: Lasso\n",
      "Loss: 1963.0720095027534\n",
      "Model name: XGBRegressor\n",
      "Loss: 1295.9473076156137\n",
      "==============================\n",
      "Best model: XGBRegressor\n",
      "Best loss: 1295.9473076156137\n"
     ]
    }
   ],
   "source": [
    "fine_tune_model_args = {\n",
    "    'train_data': train_data_final, \n",
    "    'target_col': target_col, \n",
    "    'cvs': cvs, \n",
    "    'scoring': scoring, \n",
    "    'timestamp_col': timestamp_col, \n",
    "    'optuna_n_trials': optuna_n_trials\n",
    "}\n",
    "\n",
    "arima_model = ARIMAModel()\n",
    "lasso_model = fine_tune_model('lasso', **fine_tune_model_args)\n",
    "xgb_model = fine_tune_model('xgboost', **fine_tune_model_args)\n",
    "# rf_model = fine_tune_model('random_forest', **fine_tune_model_args)\n",
    "\n",
    "models = [arima_model, lasso_model, xgb_model]\n",
    "evaluate_models(models, train_data_final, test_data_final, target_col, timestamp_col, scoring, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82dfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
